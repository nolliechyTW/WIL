# Content
- Caching Benefits
- Cache Eviction Policies
- Cache Invalidation
- Caching Strategies
- Caching Development
- Caching Mechanisms
- Content Delivery Networks
- Open Source Caching Solutions
- Conclusion

## Voc 
- When data is accessed from a cache, there are two possible outcomes: **cache hit** and **cache miss**. A cache hit occurs when the requested data is found in the cache, allowing for fast retrieval without accessing the slower main memory or external resources. On the other hand, a cache miss happens when the requested data is not present in the cache, requiring the system to fetch the data from the main memory or external storage.
- **Cache hit rates** measure the effectiveness of the cache in serving requests without needing to access slower external storage, while **cache miss rates** indicate how often the cache fails to serve requested data.

## Caching Benefits
- Caches offer **faster access** times compared to main memory or external storage.
- By serving data from a cache hit, the system avoids the delay associated with fetching data from main memory or external sources, thereby **reducing overall latency**.
- Caches help **optimize bandwidth usage** by reducing the number of requests sent to slower storage. 
    - Bandwidth usage: the amount of data transmitted over a network connection within a specific time period.
- Caches **improve overall system throughput** by allowing the CPU to access frequently needed data quickly.
- Caches exploit the **Pareto distribution (80/20 rule)** by storing the small fraction of data that accounts for the majority of access requests, ensuring quick retrieval for the most critical operations.

## Cache Eviction Policies
- Cache eviction policies try to **maximize the cache hit ratio**—the percentage of time the requested item was found in the cache and served. 
- There are various caching policies, including Belady’s algorithm, queue-based policies (FIFO, LIFO), recency-based policies (LRU, TLRU, MRU, SLRU), and frequency-based policies (LFU, LFRU).
    - Belady’s Algorithm
        - evicts the data item that will be used furthest in the future. (usually impractical to obtain)
    - Queue-Based Policies
        - FIFO (First-In-First-Out): evicts the oldest data item from the cache.
        - LIFO (Last-In-First-Out): evicts the first data item from the cache.
        - Problem: does not consider the access pattern
    - Recency-Based Policies
        - LRU (Least Recently Used): evicts the least recently accessed data item from the cache; equires tracking access timestamps for each item.
        - MRU (Most Recently Used): evicts the most recently accessed data item from the cache. 
    - Frequency-Based Policies
        - LFU (Least Frequently Used): assumes that items with lower access frequency are less likely to be accessed in the future.
        - LFRU (Least Frequently Recently Used): combines the concepts of LFU and LRU by considering both the frequency of access and recency of access. It evicts the item with the lowest frequency count among the least recently used items.
    - Allowlist Policy: defines a set of prioritized items eligible for retention in a cache when space is limited.

## Cache Invalidation
## Caching Strategies
## Caching Development
## Caching Mechanisms
## Content Delivery Networks
## Open Source Caching Solutions
## Conclusion